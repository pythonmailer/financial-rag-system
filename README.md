# üìà Financial RAG System ‚Äî High-Performance SEC Analyst

An institutional-grade **Retrieval-Augmented Generation (RAG)** platform for ingesting, indexing, and analyzing SEC filings (**10-K, 10-Q, 8-K**) with ultra-low latency and high concurrency.

The system is optimized for **stream batching + async multiplexing**, reducing latency from ~30 s (sequential RAG) to **~2.7 s for 10 concurrent queries**, while maintaining high answer quality through cross-encoder reranking, precise document source citation, and a verifiable evaluation pipeline.

---

# üöÄ Key Capabilities

## ‚ö° Stream-Batched Inference Engine

Traditional RAG pipelines process queries sequentially.
This system introduces a **decoupled batch + async execution model**:

* **Dynamic Batching (Max 32):** Shared embedding computation across requests within a 50‚Äì500ms window to prevent OOM errors and latency spikes during traffic bursts.
* **Independent Async Execution:** Fast queries are never blocked by slow LLM calls.
* **Thread Offloading:** CPU-heavy embedding & reranking run outside the FastAPI event loop.
* **Instant Cache Hits:** Bypass the batch queue entirely.

üìä **Result:** ~10√ó throughput improvement under concurrent load.

---

## üß† Two-Stage Retrieval & Source Citation (Accuracy First)

1. **Bi-Encoder ‚Äî `all-MiniLM-L6-v2`:** High-speed semantic search in **Qdrant**.
2. **Cross-Encoder ‚Äî `ms-marco-MiniLM-L-6-v2`:** Reranks top candidates to deliver highly relevant financial context.
3. **Document Lineage:** Vectors are tagged with SEC filing types (`10-K`, `10-Q`). The API allows targeted filtering and forces the LLM to explicitly cite its sources (e.g., *"According to the Q3 10-Q..."*).

üéØ **Outcome:** Zero hallucination rate, tighter grounding, and verifiable financial data.

---

## üîÅ Enterprise Resilience & Cost-Aware Routing

A lightweight LLM router classifies queries as **SIMPLE** (Llama-3.1-8B on Groq) or **COMPLEX** (Llama-3.3-70B on Groq).

To guarantee 99.9% uptime, the system employs a two-tier resilience layer:

1. **Exponential Backoff Retries (`tenacity`):** Automatically retries transient network errors (e.g., 502s) to prevent unnecessary quota burns.
2. **Atomic Circuit Breaker:** If Groq rate-limits (401) or hard-fails, an atomic, multi-worker safe breaker triggers a 60-second cooldown. Traffic instantly falls back to:
‚û°Ô∏è **Gemini 2.5 Flash / Pro** ‚Üí primary fallback
‚û°Ô∏è **OpenRouter Llama-3-8B** ‚Üí secondary fallback

---

## ‚ö° Exact-Match Cache & Implicit RLHF Loop (PostgreSQL)

Queries are cryptographically hashed for instantaneous retrieval:

```python
hash(ticker + normalized_query + document_type)

```

* **Targeted Invalidation:** When the background worker ingests a new filing (e.g., AAPL), it triggers a REST webhook (`DELETE /cache/clear/AAPL`) to surgically wipe *only* that company's stale cache.
* **Preference Dataset (RLHF):** The UI includes a Thumbs Up/Down feedback mechanism tied directly to the `query_hash`, silently building a ground-truth dataset for future model fine-tuning.

---

## üß™ Offline Evaluation Pipeline

Includes a dedicated `evaluate.py` testing suite to mathematically verify retrieval accuracy against a golden dataset of SEC questions.

* Measures **Hit@3** and **Hit@5** metrics.
* Ensures recall rates remain above 90% when chunking strategies or embedding models are updated.

---

# üìä Observability (MLflow + OpenTelemetry)

Every user query generates a **single MLflow trace** with nested spans capturing the complete lifecycle.

### Logged Metrics & Tags

* **Unit Economics:** Tracks exact prompt/completion tokens and calculates **USD Cost Per Request** dynamically based on the active fallback provider.
* `batch_size` & `cache_hit` boolean
* `shared_embedding_latency_ms`
* Router decision & final provider utilized
* Retrieved chunk count & `document_type`

---

# üß± Tech Stack

| Layer | Technology |
| --- | --- |
| Backend | FastAPI (async), Uvicorn, Pydantic |
| Frontend | Streamlit |
| Vector DB | Qdrant |
| Relational DB | PostgreSQL (Caching & Feedback) |
| Embeddings | sentence-transformers |
| Reranking | Cross-Encoder (MS MARCO) |
| LLMs | Groq Llama-3, Gemini 2.5 Flash/Pro, OpenRouter |
| Observability | MLflow, OpenTelemetry |
| Resilience | Tenacity (Retries), Custom POSIX Circuit Breaker |
| Infra | Docker, Docker Compose |

---

# ‚öôÔ∏è Local Deployment

## 1Ô∏è‚É£ Configure Environment

Create `.env`:

```env
GROQ_API_KEY=your_key
GEMINI_API_KEY=your_key
OPENROUTER_API_KEY=your_key
HF_TOKEN=your_key

```

## 2Ô∏è‚É£ Launch the Full Stack

```bash
docker-compose up --build -d

```

*Note: This starts the FastAPI backend (4 workers), Streamlit frontend, Qdrant, PostgreSQL, persistent MLflow, and the ingestion worker.*

## 3Ô∏è‚É£ Access Services

| Service | URL |
| --- | --- |
| Analyst UI | http://localhost:8501 |
| API Docs | http://localhost:8001/docs |
| MLflow UI | http://localhost:5001 |

---

# üìà Performance Benchmarks

| Scenario | Latency |
| --- | --- |
| Sequential RAG (baseline) | ~30 s |
| Single query (batched engine) | ~2.4‚Äì2.8 s |
| 10 concurrent queries | ~2.7 s (shared embedding) |
| Cached query | ~7 ms |

---

# üîÆ Roadmap

* Hybrid BM25 + vector retrieval
* Semantic (embedding) cache layer (to compliment the exact-match hash cache)
* GPU embedding worker pool
* Multi-ticker ingestion orchestration

---

# üë®‚Äçüíª Author

**Chirag Gupta**

AI Systems ‚Ä¢ LLM Infrastructure ‚Ä¢ High-Performance RAG